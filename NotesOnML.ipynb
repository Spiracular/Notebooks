{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias\n",
    "[Video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7)\n",
    "\n",
    "## Types of Bias\n",
    "- Selection Bias\n",
    "- Publication Bias\n",
    "- Non-response bias\n",
    "- Length bias\n",
    "\n",
    "Bias is difference between estimated and the average estimator.\n",
    "\n",
    "$$ Bias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$$\n",
    "\n",
    "[Mean Squared Error](http://scott.fortmann-roe.com/docs/BiasVariance.html): There is a trade off between bias and variance.\n",
    "\n",
    "$$ MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) $$\n",
    "\n",
    "## Fisher Weighting\n",
    "\n",
    "How do we combine multiple independent unbiased estimators into one estiate?\n",
    "\n",
    "$$\\hat{\\theta} = \\sum_{i=1}^k w_i \\hat{\\theta}_i $$\n",
    "\n",
    "Weights sum to 1, with each weight inverselely proportional to variance.\n",
    "\n",
    "(Presumably Eigenvectors could come into play here to accomodate this?)\n",
    "\n",
    "[FiveThirtyEight senate forecasting link](http://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/)\n",
    "- Exponential decay based on recency of poll\n",
    "- Sample size of poll\n",
    "- Pollster rating\n",
    "\n",
    "Multiple Testing, Bonferroni (fishing for correlation): How do you handle p-values when testing multiple hypotheses? Simple test is Bonferroni: divide significance level by number of hypotheses being tested.\n",
    "\n",
    "Minimizing MSE in regression line.\n",
    "\n",
    ">Kahneman Quote\n",
    "<small>\"I had the most satisfying Eureka experience of my career attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning... 'On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don't tell us that reinforcement works and punishment does not'... This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them. \" </small>\n",
    "\n",
    "## Regression Paradox\n",
    "\n",
    "Inheritance of height & regression towards mean: weighted average of father's height and average height to get son's height estimate.\n",
    "\n",
    "BUT! If you're predicting the father's height from the son's, you ALSO add in a weighted copy of the average height.\n",
    "\n",
    "In both cases you have regression towards the mean, forward and backward.\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "Criterion: Ordinary Least Squares(OLS) = Scoring w/ MSE\n",
    "\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "\n",
    "Typically more variables than categories, but classic regression breaks down when categories are comparable or higher in number than variables.\n",
    "\n",
    "## Sample Quantities vs. Population Quantities\n",
    "\n",
    "Minimize OLS for sample, where x and y are data vectors...\n",
    "\n",
    "$$\\hat{\\beta}_0 = y-\\hat{\\beta}_1\\bar{x}$$\n",
    "\n",
    "For population (think of x & y as rvs)...\n",
    "\n",
    "$$y=\\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "$$E(y) = \\beta_0 + \\beta_1 E(x)$$\n",
    "\n",
    "$$cov(y,x) = \\beta_1 cov(x,x)$$\n",
    "\n",
    "Residuals as projection in LA; find point in column space of x (all sets of x coordinates?) closest to the y point?\n",
    "\n",
    "Conditional Expectation: Find the function of X that best predicts Y.\n",
    "\n",
    "Gauss-Markov Theorem relative: If error is normal dist, expect linear to minimize MSE.\n",
    "\n",
    "Residuals: leftover error $\\epsilon$?\n",
    "<center>from</center>\n",
    "$$y = \\beta X + \\epsilon$$\n",
    "\n",
    "Plot residuals vs. fitted values and vs. predictor variable! Hoping for no pattern.\n",
    "\n",
    "Increasing residuals: Heteroscedascity (different variances), which means you should be using other estimators.\n",
    "\n",
    "R^2 measures explained variance, but not a good measure of prediction power.\n",
    "\n",
    "$$R^2 = var(X\\hat{\\beta}) / var(y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Regression Cont.](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=664f668e-e008-4f44-8600-e09ee6d629b0)\n",
    "\n",
    "For some continuous predictor values, you may want to take logs before regression (loglinear?)\n",
    "\n",
    "Do not include all possible outcomes in coefficient analysis because colinearity? (will explain later)\n",
    "\n",
    "## Colinearity\n",
    "\n",
    "Avoid having predictor variables that are highly corellated with each other. Results in instability, high variance in estimates, and worse interpretability.\n",
    "\n",
    "Predicting a binary response (logistic regression)\n",
    "\n",
    "[Free book: Intro to Stat Learning (w/ R)](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "Odds ratio (...I still suspect effect size is better, the one using STDev of treatment result compared to control.)\n",
    "\n",
    "Confounding factors (unaccounted correlates of both input and output variable)\n",
    "\n",
    "(Aortic regurgitation sounds so gross; context of Fen/Phen and heart disease)\n",
    "\n",
    "\n",
    "<small>Look up: Probit regression for binary predictors (normal dists)</small>\n",
    "\n",
    "logit = log of the odds? Calculating probability for a binary predictor.\n",
    "\n",
    "$$logit(p) = ln (\\dfrac{p}{1-p}) = \\beta_0 + \\beta_{1...k} X_{1...k}$$\n",
    "\n",
    "(Estimate Betas with MLE or something)\n",
    "\n",
    "Adjusted odds ratio\n",
    "\n",
    "$$\\beta_{fen} = logit(p_A) - logit(p_B) = ln \\left( \\dfrac{ \\dfrac{p_A}{1-p_A} }{ \\dfrac{p_B}{1-p_B} } \\right) $$\n",
    "\n",
    "(One line of additional practice with LaTex)\n",
    "\n",
    "$$ ln \\Bigg( \\dfrac{ \\dfrac{p_A}{1-p_A} }{ \\dfrac{p_B}{1-p_B} } \\Bigg) $$\n",
    "\n",
    "And back to calculating the \"adjusted\" odds ratio...\n",
    "\n",
    "$$Adjusted \\: Odds \\: Ratio = e^{\\hat{\\beta}_{treatment}}$$\n",
    "\n",
    "(the odds ratio for 2 people with all other known factors held constant)\n",
    "\n",
    "Curse of Dimensionality: For a uniformly random point in a box with side length 2, what is the probability that the point is in the unit ball?\n",
    "\n",
    "It goes way down as you add dimensions...\n",
    "\n",
    "Interpolation vs. Extrapolation (Interpolation: predict something in the middle of other points, Extrapolation: extending towards outlier points. Interpolation usually much more reliable.)\n",
    "\n",
    "Tall data vs. Wide data? (Large sample size vs. Large number of predictor variables)\n",
    "\n",
    "Traditional stats are usually geared towards tall data, curse of dimensionality in the case of wide data. Assume n > p.\n",
    "\n",
    "Wide datasets increasingly common: neuroimaging, microarrays, MOOCs, etc.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "$$\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "$$RSS + \\lambda\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "We add a penalty to try to shrink betas towards the origin. Essentially, disallows extremely large beta values.\n",
    "\n",
    "(Value of lambda determined by testing and use of cross-validation)\n",
    "\n",
    "## Shrinkage Estimation\n",
    "\n",
    "### Stein's Paradox\n",
    "For multiple values of y through k >= 3, and N() = Normal Dist. and assuming independents.\n",
    "\n",
    "$$y_k \\sim N(\\theta_k, 1)$$\n",
    "\n",
    "How do you estimate the vector $\\theta$ under squared error loss?\n",
    "\n",
    "Lets initially predict that $\\theta_1 = y_1$ and so on. It's unbiased, best unbiased estimated, Max Likelihood Estimator, etc... but it's inadmissible! There is always an estimator that would do better and have less expected loss. It's the following...\n",
    "\n",
    "$$\\theta_j = \\left( 1 - \\dfrac{k-2}{\\sum_i{y_i^2}} \\right)y_j$$\n",
    "\n",
    "This is true EVEN THOUGH THESE PROBLEMS ARE COMPLETELY UNCORRELATED!\n",
    "\n",
    "(Shocked statistical minds, laid groundwork for heirarchical modeling and statistical Bayes)\n",
    "\n",
    "James-Stein Estimators (Baseball players and shrinkage estimation): Shrink all estimates towards the grand mean (~regression towards mean). Significantly better squared error!\n",
    "\n",
    "## LASSO and Sparsity\n",
    "\n",
    "Different penalty term. Minimized function is not squared residuals. It is the following:\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j| $$\n",
    "\n",
    "(Main unfortuate property: Sharp corner, absolute values causes probs with ordinary calculus.)\n",
    "\n",
    "This helps induce *sparsity*. It will kill off some of the $\\beta$ values, reduce them to 0.\n",
    "\n",
    "Visual: LASSO w/ diamond around origin, Ridge w/ circle around origin.\n",
    "\n",
    "Diamond constraints developed from calculating the values where the sum of abs values is less than a constant, circle from sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification, kNN, Cross-validation, Dimensionality Reduction\n",
    "\n",
    "MIT Scene Recognition Demo\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "Input: Training set of N data points, labeled with one of K different classes\n",
    "\n",
    "Machine Learning\n",
    "\n",
    "Evaluation vs. test set: Compare true labels (aka ground truth) to those predicted by classifier.\n",
    "\n",
    "**Supervised Learning**: kNN, SVM, Decision Trees, Random Forests, Bagging, Boosting, etc.\n",
    "\n",
    "- Makes predictions for new data points\n",
    "- Data has labels (categories)\n",
    "\n",
    "**Unsupervised Learning**: PCA, MDS, Clustering\n",
    "\n",
    "- Find patterns in the data\n",
    "- Data hs no labels\n",
    "\n",
    "Feature selection is important; some things that are very similar in one set of features maybe cleanly distinguishable in another. ex: apples and oranges have similar weight, but very different color.\n",
    "\n",
    "## Nearest Neighbor\n",
    "\n",
    "Basically a look-up, identity decided by majority vote of nearby classified examples.\n",
    "\n",
    "Image: Voronoi diagram around points, shows area around a point that would be considered closer to that point than to any other.\n",
    "\n",
    "1-NN\n",
    "- Simple, and quite good for low dimensions\n",
    "- Rough decision boundary and possibility of islands\n",
    "- Training complexity? O=1 to add item to training set\n",
    "- Testing complexity? O=m*n\n",
    "\n",
    "^ Training is no work, testing holds all the complexity.\n",
    "\n",
    "- Variance: How much does the boundary change when you add data/change dataset? (fairly high for 1NN, lower for k-NN)\n",
    "- Bias: low for 1-NN, increases as you increase k-NN\n",
    "\n",
    "For overly large k, boundary potentially becomes too smooth.\n",
    "\n",
    "Higher k lowers variance, increases bias\n",
    "\n",
    "(k is a \"hyperparameter\")\n",
    "\n",
    "Couple different distance metrics possible (default = Euclidian)\n",
    "\n",
    "Folding training data:\n",
    "\n",
    "Training data: Majority of folds (1-4)\n",
    "\n",
    "Validation data: Estimate hyperparameters (fold 5)\n",
    "\n",
    "Test data: Measure performance (final chunk of data)\n",
    "\n",
    "Only use the test data at the end!\n",
    "\n",
    "CIFAR-10 Data Set: large number of small images (32x32px)\n",
    "Training set: 50k images\n",
    "Test set: 10k images\n",
    "\n",
    "Turn high-dimensional data into a vector\n",
    "\n",
    "32x32x3 = ~3k dimensions\n",
    "\n",
    "## Distance metrics\n",
    "\n",
    "**Manhattan Distance**\n",
    "\n",
    "L1\n",
    "\n",
    "$$d_1(I_1, I_2) = \\sum_p |I_1^p - I_2^p|$$\n",
    "(p=pixel, I=image)\n",
    "\n",
    "**Euclidian Distance**\n",
    "\n",
    "L2\n",
    "\n",
    "$$d_1(I_1, I_2) = \\sqrt {\\sum_p (I_1^p - I_2^p)^2}$$\n",
    "\n",
    "**General Lp norms** (?)\n",
    "\n",
    "Lp\n",
    "\n",
    "$$ ||x||_p = (|x_1|^p + ... + |x_n|^p)^{1/p} $$\n",
    "$$p \\leq 1,x \\in {\\rm I\\!R} ^n$$\n",
    "\n",
    "Cross-validation accuracy: predictions vs. test data\n",
    "\n",
    "Best classifiers vs. images: ~95% accuracy, NNet-based (humans get 94%)\n",
    "\n",
    "SIFT & Object Recognition by features (shift &  rotation invariant)\n",
    "\n",
    "Self-driving cars with all of: Lidar, Video, and Radar\n",
    "\n",
    "At large amounts of diverse data, sparsity kicks in and you get the \"curse of dimensionality.\" (which really messes up NN classification)\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Trying to retain distance?\n",
    "\n",
    "High-dimensional data: Images, Documents, Gene expression, MEG readings, etc.\n",
    "\n",
    "Project high-dimensional data into a lower-dimensional sub-space that best fits the data and preserves the distances between these data points in the higher dimensions.\n",
    "\n",
    "A lot of the transforms are linear, for simplicity of computation.\n",
    "\n",
    "Linear Methods\n",
    "\n",
    "- Does the data mostly lie in a hyperplane?\n",
    "- If so, what is the *intrinsic* dimensionality d?\n",
    "\n",
    "### PCA: Principal Component Analysis\n",
    "\n",
    "Useful for dimensionality reduction, compression, visualization\n",
    "\n",
    "MusicBox & PCA by MIT Anita Lily\n",
    "\n",
    "Transform of ~linear data: Subtract mean and rotate into original coordinate space.\n",
    "\n",
    "Choose PCs to minimize orthogonal diastance to new vector v\n",
    "\n",
    "Equivalent: put v in direction of maximum variance (max. spread along v; v holds most of the variation)\n",
    "\n",
    "Difference from Linear Regression: difference along original axes (x & y), while PCA runs for orthogonal distance (distance perpendicular to the length of the vector v). Result in different lines!\n",
    "\n",
    "Minimize projection distance from v, maximize spread along v!\n",
    "\n",
    "Steps: subtract mean, scale each dim by variance, compute S, compute k largest eigenvectors of S. These eigenvectors are the k principal components.\n",
    "\n",
    "Covariance matrix = S\n",
    "\n",
    "$$S = \\dfrac{1}{N}X^TX$$\n",
    "\n",
    "May also be computed via Singular Value Decomposition.\n",
    "\n",
    "Screeplot: plot fraction of variance explained by each PC vs. ordered PCs, work out where gains drop off.\n",
    "\n",
    "PCA w/ Handwritten Digits.\n",
    "\n",
    "Reconstructions as linear combination of 'basis' PC vectors\n",
    "\n",
    "(ex: Eigenfaces)\n",
    "\n",
    "### Multi-Dimensional Scaling (MDS)\n",
    "\n",
    "Decide on distance metric, compute pairwise distances\n",
    "\n",
    "Give it a matrix with distnace metrics between data points, make a distance matrix.\n",
    "\n",
    "Modify, then compute PCA on distance matrix\n",
    "\n",
    "Ex:keeping distances between cities from lat and long coordinates\n",
    "\n",
    "(Augh, this guy really did not share the math as much, but whatever. I guess it was Linear Algebra...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SVM and Performance Evaluation\n",
    "[SVM Video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f21fcc8f-93a8-49f6-9ff8-0f339b0728bd)\n",
    "\n",
    "## Separating Hyperplane\n",
    "\n",
    "x : data point\n",
    "y : label $\\in \\{-1,+1\\}$\n",
    "w : weight vector (orients plane)\n",
    "bias : moves origin point for plane relative to overall origin\n",
    "\n",
    "(^This set of labels makes calculating SVMS really easy)\n",
    "\n",
    "On the hyperplane...\n",
    "\n",
    "$$w^T x + b = 0$$\n",
    "\n",
    "... and to one side, the values are +1 and on the other, the values are -1.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "$$\\sum(w_i^T x_i + b) = \\hat{y}$$\n",
    "\n",
    "Mathematically describes a neuron! Basic unit of Deep Learning.\n",
    "\n",
    "1957 by Frank Rosenblatt\n",
    "\n",
    "**Side note: Step vs. Sigmoid Activation**\n",
    "\n",
    "Image: step from 0, jumps, plateaus at 1 vs. sigmoid 0 to 1\n",
    "\n",
    "$$s(x) = \\dfrac {1} {1+e^{-cx}}$$\n",
    "\n",
    "(Minsky did pretty intense critique?)\n",
    "\n",
    "### The XOR Problem\n",
    "\n",
    "$$\\begin{bmatrix}-1&1 \\\\ 1&-1\\end{bmatrix}$$\n",
    "\n",
    "No simple separating hyperplane!\n",
    "\n",
    "Solution? Add a new dimension. If that changes dots with one value, then a single separating hyperplane is possible again.\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "Good simple classifier across a wide variety of problems, but may take a little tweaking to get it to work (she wouldn't characterize it as \"off the shelf\").\n",
    "\n",
    "### Maximum Margin Classification\n",
    "\n",
    "Choose the hyperplane that provides the highest margin (distance) between the separatig hyperplane and the points closest to the boundary.\n",
    "\n",
    "**Support Vectors**: the closest points, and those that define the margin.\n",
    "\n",
    "scikit learn function : c and gamma\n",
    "\n",
    "gamma $\\gamma$ = distance \n",
    "\n",
    "Margin:\n",
    "$$x_{\\bot}^i  = x^i - \\gamma^i \\cdot \\dfrac {w} {||w||}$$\n",
    "\n",
    "$$w^T x_{\\bot}^i + b = 0$$\n",
    "\n",
    "$$\\therefore \\ \\gamma^i = y^i \\dfrac{w^T x^i + b} {||w||}$$\n",
    "\n",
    "with i...m (not sure what m is... :( )\n",
    "\n",
    "(y just determines if it's up or down under the conditions of +1 or -1)\n",
    "\n",
    "Described as: normalize w (basis orientation?) to 1 and do $\\gamma$ steps of w down to reach $x_{bottom}$\n",
    "\n",
    "Trying to compute max $\\gamma$\n",
    "\n",
    "optimizer has complication because ||w|| is non-convex, but the computer's going to deal for you! Woo.\n",
    "\n",
    "SVMs experience very disproportionate effect of points near the boundary. \n",
    "\n",
    "Take outliers, measure margin to boundary they actually lie on opposite side of. Those are slack variables\n",
    "\n",
    "Slack Variable = $\\xi_i$\n",
    "\n",
    "**C** is the constant the slack variable is multiplied by\n",
    "\n",
    "$$min \\dfrac{1}{2} ||w||^2 + C\\sum{\\xi_i}$$\n",
    "\n",
    "<center>Constrained by...</center>\n",
    "\n",
    "$$Margin: \\ \\gamma^i \\leq 1-\\xi_i$$\n",
    "\n",
    "If large C, little slack allowed. If small C, allows more misclassificaions.\n",
    "\n",
    "((Outside info: large $\\gamma$ leads to high bias low variance, low $\\gamma$ leads to reverse? Something about Gaussian params.))\n",
    "\n",
    "### XOR Problem Revised\n",
    "\n",
    "Add x^2 to measure and develop a hyperplane out of the extra dimension's parabola (assess using values of x^2)\n",
    "\n",
    "Non-linear decision boundary (blow up, get hyperplane, crunch back down and draw decision bounay)\n",
    "\n",
    "### Video\n",
    "SVM with a polynomial Kernel visualization -Udi Aharoni\n",
    "\n",
    "Quadratic Kernel\n",
    "\n",
    "Quadratic expansion instance\n",
    "\n",
    "$$x = (x_1, x_2)$$\n",
    "\n",
    "$$\\phi(x) = (1,\\sqrt{2x_1}, \\sqrt{x_2}, x^2_1, x^2_2, \\sqrt{2x_1 x_2})$$\n",
    "\n",
    "$\\phi(x) \\cdot \\phi(z)$  looks much more complicated initially (higher dimensions, so dot product of the multiple points)... but it compresses down to a simple formula!\n",
    "\n",
    "$$\\phi(x) \\cdot \\phi(z) = (1+x \\cdot z)^2$$\n",
    "\n",
    "This is the kernel trick\n",
    "\n",
    "$$K(x,z) = \\phi(x) \\cdot \\phi(z) = (1+x \\cdot z)^S$$\n",
    "\n",
    "Tune S for Polynomial kernel (may also be called D for Degree)\n",
    "\n",
    "Radial basis function (RBF):\n",
    "\n",
    "$$K(x,z) = exp(-\\gamma(x-z)^2)$$\n",
    "\n",
    "Popular, can go to infinite dimensions! Tune $\\gamma$\n",
    "\n",
    "$$w^T x + b = \\sum_{i=1}^m \\alpha_i y^i (x^i, x) + b$$\n",
    "\n",
    "Prediction speed depends on number of support vectors\n",
    "\n",
    "## [Explanation of SVN Math by Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n",
    "((Overall: did not like this presentation as much as the last one, may want to relearn it fron the Ng notes and hope it makes more sense that round.))\n",
    "\n",
    "Kernel Trick for SVM\n",
    "- Arbitrarily many dimensions\n",
    "- Little computational cost\n",
    "- Maximal margin helps vs. curse of dimensionality; limits the points you need to pay attention and compute for.\n",
    "\n",
    "Max margin, $\\gamma$ or degree $K$ for Polynomial kernel.\n",
    "\n",
    "OpenCV : C image processing, may have a good Python wraper?\n",
    "\n",
    "\n",
    "Effects of different C and gamma values\n",
    "![C and gamma image](http://scikit-learn.org/stable/_images/plot_rbf_parameters_001.png)\n",
    "\n",
    "\n",
    "## **Take-aways**\n",
    "- SVMs are **NOT** scale invariant! Assume you need to normalize beforehand (mean = 0, std = 1 or map to [0,1] or [-1,1])\n",
    "\n",
    "- Normalize the test set in the same way, and don't use it until the end!\n",
    "\n",
    "- RBF Kernel is a good default\n",
    "\n",
    "- Try exponential sequence when testing parameters\n",
    "\n",
    "Recommended Reading: Chih-Wei Hsu et al., \"A Practical Guide to Support Vector Classification\", Bioinformatics(2010)\n",
    "\n",
    "## Parameter Tuning\n",
    "\n",
    "Selecting: Kernel, parameter values, value for C\n",
    "\n",
    "Giving degrees of freedom lets training fit better, but may lead to overfitting when given in excess.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Cost vs. Gamma visualization\n",
    "\n",
    "Error measures: T+, T-, F+, F- -> tp, tn, fp, fn\n",
    "\n",
    "$$tp \\ rate = \\dfrac{tp}{tp+fn}$$\n",
    "$$fp \\ rate = \\dfrac{fp}{fp+tn}$$\n",
    "\n",
    "### ROC curve\n",
    "\n",
    "chart  false positive rate vs. true positive rate\n",
    "\n",
    "Aiming for upper-left corner\n",
    "\n",
    "(ROC = Receiver Operating Characteristic)\n",
    "\n",
    "### Precision Recall Curves\n",
    "\n",
    "Recall = True Positives\n",
    "Precision = $\\dfrac {tp}{tp+fp}$ = For positives predicted, prob that it is a true positive\n",
    "\n",
    "Works well when you have a lot of true negatives?\n",
    "\n",
    "Try to approach 1x1 corner\n",
    "\n",
    "F-measure = harmonic mean (weighted average) of precision and recall\n",
    "\n",
    "## Non-binary classification\n",
    "\n",
    "Classes > 2\n",
    "\n",
    "One vs. all for each class?\n",
    "\n",
    "Take clasification w/ greatest margin, Slow training.\n",
    "\n",
    "One vs. one: train vs. each, take majority vote. Faster training.\n",
    "\n",
    "Confusion Matrix (see: scikit learn): intend to maximize diagonal = correct classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Explanation of SVN Math by Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n",
    "[Quora answer on difference between SVM and Logistic Regression](https://www.quora.com/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression/answer/Michael-O-Church?srid=jJY)\n",
    "\n",
    "[Quora: what are C and Gamma in SVMs](https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine) and best C and Gamma found using Grid-Search?\n",
    "\n",
    "Overview:\n",
    "- **SVMs**\n",
    "    - Margins & large \"gaps\" in data\n",
    "    - Optional Margin Classifier\n",
    "        - Lagrange duality\n",
    "    - Kernels (help vs. high dimensions)\n",
    "    - SMO algorithm (efficient implementation of SVM)\n",
    "\n",
    "Augh, what is $\\theta$? (It looks like it might just be atypical notation the matrix of $\\beta$s? Yeah, looks like $\\theta_0$ was intercept, so probably yes.)\n",
    "\n",
    "$$y \\in \\{1, -1\\}$$\n",
    "\n",
    "Confirmed: $\\gamma$ is margin.\n",
    "\n",
    "$$\\hat{\\gamma}^i = y^i (w^T x + b)$$\n",
    "\n",
    "$||w||$ means magnitude of vector w\n",
    "\n",
    "w is orthogonal to the hyperplane.\n",
    "\n",
    "$w/||w||$ = unit-length vector orthogonal to the hyperplane.\n",
    "\n",
    "$$w^T (x^i - \\gamma^i \\dfrac{w}{||w||})+b = 0$$\n",
    "\n",
    "And here's the formula for the (scaling-insensitive) geometric mean.\n",
    "\n",
    "$$\\gamma^i = y^i \\dfrac{w^T x^i + b} {||w||}$$\n",
    "\n",
    "and maximize geometric margin, but gamma of training set S wrt w, b is defined as the lowest $\\gamma^i$ in the set of values given.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Decision Tree and Random Forests](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=8892a8b7-25eb-4bc5-80b6-47b9cf681a05)\n",
    "\n",
    "Overview:\n",
    "- Tree classifier\n",
    "- Bagging\n",
    "- Random Forest\n",
    "\n",
    "Devision trees:\n",
    "- Fast training and prediction\n",
    "- Easy to understand and interpret\n",
    "- Problems: Overfitting-prone and only can do euclidian splits\n",
    "\n",
    "$\\theta$= threshold (decision-making boundary)\n",
    "\n",
    "\n",
    "**Weakness**: Spaces assigned to categories as rectangles; can be a limitation.\n",
    "\n",
    "**Stengths**: Normalizing not necessary (reasons obvious), Simple to multiclass.\n",
    "\n",
    "Choices:\n",
    "- Feature to examine\n",
    "- Threshold to set\n",
    "\n",
    "## Gini Impurity\n",
    "Expected error if you randomly choose a sample, and predict the class of the entire node from it.\n",
    "\n",
    "$I_g$\n",
    "\n",
    "similar results to entropy? (reasonably well correlated)\n",
    "\n",
    "gini impurity of node\n",
    "\n",
    "Given parent node A and child nodes B and C:\n",
    "\n",
    "$$\\Delta I_G(A) - \\dfrac{N(B)}{N(A)}I_G(B) - \\dfrac{N(C)}{N(A)I_G(C)}$$\n",
    "\n",
    "Misclassification\n",
    "\n",
    "$$\\dfrac{1}{N} \\sum_i^N 1 (\\hat{y_i} \\neq y_i)$$\n",
    "\n",
    "^Function is non-differentiable\n",
    "\n",
    "Gini approves of cutting off chunks with large numbers of points inside.\n",
    "\n",
    "Steps\n",
    "\n",
    "1. Check if finished\n",
    "1. For each feature $x_i$:\n",
    "    - Calculate gain from splitting on $x_i$\n",
    "    - Let $x_{best}$ be hte features with the highest gain\n",
    "1. Create a decision node that splits on $x_{best}$\n",
    "1. Repeat on the sub-nodes\n",
    "\n",
    "Stopping conditions\n",
    "- Node w/ only 1 class\n",
    "- Node w/ $<x$ data points\n",
    "- Max node depth reached\n",
    "- Adequate node purity\n",
    "- Starting to overfit (cross-validation)\n",
    "\n",
    "Decision trees tend to overfit, so we do Tree Pruning (remove a branch, see how the decision tree still fares, give it a ratio at that node)\n",
    "\n",
    "SVD: Singular Value Decomposition. Is similar to PCA? Will come up later.\n",
    "\n",
    "Netflix Prize winners took average of 800 different algorithms -and ensemble approach- to derive recommendations. (That... sure vindicates a lot of what Oli has said about Fermi Modeling with multiple plausible algorithmic theories, and pooling over their results.)\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "Learning multiple diverse trees?\n",
    "\n",
    "### Bootstrap\n",
    "Resample from your training data (draw w/ replacement)\n",
    "\n",
    "This will be useful for generating new decision trees.\n",
    "\n",
    "Can I do cross-validation on bootstrapped data? **NO!** (overlap!)\n",
    "\n",
    "Prob of not choosing n in N draws:\n",
    "\n",
    "$$p(n \\in Z^{*i}) = 1-(1-\\dfrac{1}{N})^N) = 1-e^{-1} = 0.632...$$\n",
    "\n",
    "### Bagging\n",
    "**Bagging** = **B**ootstrap **agg**regat**ing**\n",
    "\n",
    "Sample w/ replacement, learn a classifier to each bootstrap sample, and average the results.\n",
    "\n",
    "First couple of bootstrap samples do tend to significantly reduce misclassification.\n",
    "\n",
    "Bagging is **NOT** useful for linear models.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Each tree builds from a bootstrap sample\n",
    "\n",
    "Node splits calculated from *random feature subsets*\n",
    "\n",
    "(XBox Kinect used RandomForest, may be using something else now)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics and Naive Bayes\n",
    "[Video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=233f6c34-306f-481b-8ea5-be33076eb6a8)\n",
    "\n",
    "Conroversial for about 2 centuries.\n",
    "\n",
    "\n",
    "Some books on Python and Bayes:\n",
    "- Simple book called Think Bayes\n",
    "- Probabilistic Programming & Bayesian Methods for Hackers\n",
    "- Doing Bayesian Data Analysis\n",
    "- Bayesian Data Analysis: Very modern treatment\n",
    "\n",
    "\n",
    "## Bayes Formula\n",
    "$$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "P(A) = Prior\n",
    "\n",
    "P(A|B) = Posterior\n",
    "\n",
    "Coherent: You can sequentially add information, or you can add it all at once, and you still get a consistent result.\n",
    "\n",
    "($\\theta$ = hypothesis/model or similar?)\n",
    "\n",
    "$$p(\\theta | y) = \\dfrac{p(y | \\theta)p(\\theta)}{p(y)}$$\n",
    "\n",
    "Treating data y as fixed, and $L(\\theta)$ = $p(y|\\theta)$...\n",
    "$$p(\\theta | y) \\propto L(\\theta)p(\\theta)$$\n",
    "\n",
    "Bayes rule says that the posterior density $\\propto$ Likelihood function * prior density.\n",
    "\n",
    "(In instances where maximum likelihood function is stupid, sometimes shrinkage or regularization help)\n",
    "\n",
    "Frequentists don't like giving $p(\\theta)$ a probability distribution, they want to treat it as a constant, plus controversies about the existence of priors.\n",
    "\n",
    "To be fair: if two people's priors are reasonable and you feed data in, the posteriors will converge over time.\n",
    "\n",
    "### Classifier Types\n",
    "\n",
    "**Discriminative:** Just model $p(y|x)$ (ex: classic logistic regression)\n",
    "\n",
    "**Generative:** Give a full probability model $p(x,y) = p(x)p(y|x) = p(y)p(x|y)$ (ex: )\n",
    "\n",
    "## Naive Bayes Spam Filter\n",
    "\n",
    "Denominator (Maginal Likelihood?) is usually fairly hard to calculate.\n",
    "\n",
    "Naive Bayes assumes conditional independence between different pieces of evidence.\n",
    "\n",
    "Alt naming for Bayesian Analysis: Full Probability Modeling?\n",
    "\n",
    "\n",
    ">\"The process of Bayesian data analysis acan be idealized by dividing it into the following 3 steps:\"\n",
    "1. Setting up a full probability model: joint probability distribution for all observable and unobservable quantities in a problem.\n",
    "1. Conditioning on observed data: Calculate and interpret the appropriate posterior distribution and get conditional probability distribution for the unobserved quantities of interest given the observed.\n",
    "1. Evaluate the fit of the model and the implication s of the resulting posterior distribution.\"\n",
    "\n",
    ">**Gelman et al, Bayesian Data Analysis**\n",
    "\n",
    "Lets you generate fake data from your model and do a sanity check on it.\n",
    "\n",
    "His preference: use cross-validation (often emphasized by frequentists), use Bayes, don't use excessively strong priors, perform sanity-checks.\n",
    "\n",
    "### Conjugate Priors: Beta-Binomial\n",
    "\n",
    "$$X|p \\sim Bin(n,p)$$\n",
    "$$p \\sim Beta(a,b)$$\n",
    "\n",
    "$$f(p) \\propto p^{a-1}(1-p)^{b-1}$$\n",
    "\n",
    "Beta is used for mathematical convenience?\n",
    "\n",
    "**Conjuate Prior:** If you start out in a family of distributions, then observe some data, you still end up in that prior's distribution type (with possibly edited parameters), the **prior and posterior are called conjugate distributions** and the prior is a **conjugate prior** for the **Likelihood function**.\n",
    "\n",
    "Posterior p: $p|X = x \\sim Beta(a+x, b+n-x)$\n",
    "\n",
    "Expected value of Beta = $\\dfrac{a}{a+b}$\n",
    "\n",
    "### Conjugate Priors: Normal-Normal\n",
    "Normal is conjugate to itself\n",
    "\n",
    "(Stein's Theorem and heirarchical models? Multiple leves of hyperparameters (potetially infinite?))\n",
    "\n",
    "$$y|\\mu \\sim Norm(\\mu,\\sigma^2)$$\n",
    "$$\\mu \\sim Norm(\\mu_0, \\tau^2)$$\n",
    "\n",
    "Then...\n",
    "\n",
    "$$\\mu|y \\sim Norm \\left( (1-B)y + B\\mu_0\\dfrac{1}{ \\tiny{ \\dfrac{1}{\\sigma^2} + \\dfrac{1}{\\tau^2} } } \\right)$$\n",
    "\n",
    "Reminder: Fisher Weighting where the different unbiased estimators were weighted inversely to the variances.\n",
    "\n",
    "Image: Diagram for conjugacy\n",
    "\n",
    "![Conjugacy diagram](http://www.johndcook.com/conjugate_prior_diagram.png)\n",
    "\n",
    "MCMC (Markov-Chain Monte Carlo) will come up later in course (CS109?) helps w/ estimating parameters for fancier models.\n",
    "\n",
    "### Example: Ranking Reddit Comments\n",
    "\n",
    "Priors in Bayes are a bit like automatic regularization.\n",
    "\n",
    "Upvote number as Bin(n,p)\n",
    "\n",
    "Conjugate prir p ~ Beta(a,b), $pdf \\propto p^{a-1}(1-p)^{b-1}$\n",
    "\n",
    "Posterior p | data ~ Beta(a+#upvotes, b+#downvotes)\n",
    "\n",
    "Alternative method (ad-hoc, though): Pseudocounts, ex: Agresti-Coull method, where you add 2 successes and 2 failures to each instance.\n",
    "\n",
    "Interval in Bayes = Credible Interval = Probability Interval (more usefully defined than Frequentist confidence intervals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentiment Classification Using sklearn](https://www.youtube.com/watch?v=y3ZTKFZ-1QQ&feature=youtu.be)\n",
    "by Ryan Rosario: Quant Engineer at FB, ML & NLP\n",
    "\n",
    "?Real-time Pitting?\n",
    "\n",
    "From PyData2014\n",
    "\n",
    "?NLTK? (Natural Language Tool Kit)\n",
    "\n",
    "Goal\n",
    "Input: text\n",
    "Output: general state (ex: positive, negative), specific emotion/sense w/ text\n",
    "\n",
    "## General Sentiment Analysis Approaches\n",
    "\n",
    "Common Lexicon-based Approaches\n",
    "(alternatives: Dictionary, ML: SVM, Naive Bayes)\n",
    "- SentiWordnet\n",
    "- SenticNet\n",
    "- WordNet-Affect\n",
    "- LIWC: Linguistic Inquiry and Word Count\n",
    "\n",
    "(ML approaches may use ratios, lexical features like quantifiers & negations)\n",
    "\n",
    "Deep Learning w/ 80-85% accuracy combines bag-of-words w/ lexical structure (Socher et al 2013). Trained on labeled **Stanford Sentiment Treebank**. Recursive Neural Network (Recursive Neural Tensor Network). Not particularly scalable; requires large samples.\n",
    "\n",
    "Words like 'not' can flip sentiment of the rest, very can exaggerate sentence.\n",
    "\n",
    "Subjunctive mood? Discusses things you doubt, regret, wish, etc. (not in English, but other languages)\n",
    "\n",
    "### SentiWordNet Adapatation\n",
    "\n",
    "SynSets: words divided into different meanings in Wordnet corpus: positivity vs. negativity vs. objectivity scoring\n",
    "Very & too to amplify, not to reverse.\n",
    "\n",
    "Lexical analysis:\n",
    "- Very slow\n",
    "- Not very accurate (~65%, 0.3 F1 score)\n",
    "- Varies by language (not scalable)\n",
    "- Human judges are unreliable (only around 20% agreement?)\n",
    "\n",
    "Use words? Train a classifier?\n",
    "\n",
    "(FB gets pre-labeled text w/ expressions. Ha!)\n",
    "\n",
    "(Heh, *mime smiley* is a negative emotion, even when the guy turning invisible is considered neutral)\n",
    "\n",
    "90 days of data at a time & millions of peices of emotional content tagging.\n",
    "\n",
    "Use Chi-squared to select top K features (faster if you use the hashing trick)\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\dfrac{(O_i - E_i)^2}{E_i}$$\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "with Bag of Words\n",
    "\n",
    "Performs okay across a broad array of applications, good first-test algorithm, assumes all features are independent (which rarely holds).\n",
    "\n",
    "$$P(C|F) = \\dfrac{\\prod_{i=1}^n{P(F_i|C)}}{P(C)}$$\n",
    "\n",
    "With $<40%$ = negative, $>60%$ = positive\n",
    "\n",
    "- Supports multiple languages and locales!\n",
    "- Accuracy: 77-83%, F1 score: 0.79-0.84\n",
    "- F+ rate: 7-22%, F- rate: 9-24%\n",
    "\n",
    "(ROC curve: Turks were straightforward, French & Italian were harder to classify)\n",
    "\n",
    ">## Python Libraries\n",
    "- Hive, Presto, and Dataswarm(Py-based data pipelines?) for data acquisition\n",
    "- pandas for managing relational data\n",
    "- sklearn for text processing and ML\n",
    "- numpy as dependency\n",
    "- scipy as dependecy for working with sparse matrices\n",
    "\n",
    "Pandas: simple tabular data struct, fast (w/ np), good with other np arrays ex: sklearn, easy I/O for debugging\n",
    "\n",
    "sklearn: uniform interface for training many different classifiers, fast vectorization of text, supports incremental training (adding new batches to current model)\n",
    "\n",
    "(spelling out vectorization if there wasn't a function for it is just painful)\n",
    "\n",
    "- Read\n",
    "- Split (train, test (, validate?))\n",
    "- Vectorize text & take care of stop-words\n",
    "- Clean result data (ex: to {0,1})\n",
    "- Define classifier\n",
    "- Fit classifier\n",
    "- Get predicted sentiment, probability\n",
    "- classifier.score accuracy vs. test; precision, recall, f1 via sklearn.metrics.precision_recall_fscore_support\n",
    "- Print-outs\n",
    "- Matplotlib plotting\n",
    "\n",
    "(Quick note: ROC curve calculation, manually: 45mins to run, built-in function: 10 secs ($>2$ orders of magnitude difference).)\n",
    "\n",
    "?classifier: SGD?\n",
    "\n",
    "Dataswarm pipeline in cloud?, Train/Test Divde (70-30%?)\n",
    "\n",
    "pickled, or dumped to disk w/ joblib (external package in sklearn)\n",
    "\n",
    "Feature selection, Training, Model\n",
    "\n",
    "Testing\n",
    "\n",
    "Output: MySQL and/or Dashboards\n",
    "\n",
    "Some other tools they have: Hive transformer (stdin to stdout transform), Thrift service for classification\n",
    "\n",
    "### Questions from talk:\n",
    "Q: chi-squared, univariate vs. multivariate, if uni how do you handle correlations\n",
    "A: auto 2-sided, Audrey show (?) w/ one-sided worked poorly\n",
    "Q: Word-stemming?\n",
    "A: Debate on usefulness of stemming, felt wasn't needed here and didn't use it.\n",
    "Q: What percent of data uses emoticons?\n",
    "A: Unknown\n",
    "Q: Additional resolution on emotion felt?\n",
    "A: Some emotions are much harder to predict than others\n",
    "Q: Highest weights?\n",
    "A: Happy, sad, love, hate, blessed, birthday, church, God, ill, sick etc.\n",
    "Q: Tracking number of appearances of high-relevance words in post?\n",
    "A: Nope\n",
    "Q: [Vowpal Wabbit mentioned](https://github.com/JohnLangford/vowpal_wabbit/wiki) (not looking too much into it for now)\n",
    "A: He likes it, this isn't Python\n",
    "Q: Are some words not emotionally tagged, ex: grocery?\n",
    "A: Don't know?\n",
    "Q: Sparseness of matrix?\n",
    "A: Very. Very very. It's text, this is normal. Trivially sparse.\n",
    "Q: Handle mispellings to reduce sparsity?\n",
    "A: Probably dropped out for rare ones, kept for common ones like committment\n",
    "?ngrams? ?unigrams? (oh, single words not 2-word phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Best Practices in Supervised Learning](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=b33eec92-d049-4353-a904-5054eb718aff)\n",
    "\n",
    "## Overview\n",
    "Clasifier Wrapup:\n",
    "- Some RF things\n",
    "- Regression\n",
    "\n",
    "ML best practices\n",
    "- Model choice\n",
    "- Imbalanced data\n",
    "- Missing values\n",
    "\n",
    "Recommender Systems\n",
    "- Collaborative filtering\n",
    "- Content-based filtering\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Builds on idea of bagging, w/ each tree generated from a bootstrap sample and node splits calculated from random  subsets of features.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "All fully-grown, unpruned trees\n",
    "\n",
    "Parameters:\n",
    "- Number of trees\n",
    "- Number of features\n",
    "\n",
    "Error depends on correlation between trees (hopefully low) and strength of single trees (hopefully high).\n",
    "\n",
    "Raise numebr of features for each split...\n",
    "- Raises correlation\n",
    "- Increases single tree strength\n",
    "\n",
    "### Out of Bag Error\n",
    "Making use of the data held back from a tree to diversify can be reused as data for test sample.\n",
    "\n",
    "Measure the prediction error of random forests by bagging to training data not used for each tree. Out Of Bag error = mean predicted error / training sample for trees which did not use that data in their bootstrap sample.\n",
    "- Very similar to cross-validation\n",
    "- Measured during training\n",
    "- May be overfitted\n",
    "\n",
    "### Variable importance\n",
    "- Use out-of-bag samples\n",
    "- Predict class for samples\n",
    "- Randomly permute values of a feature\n",
    "- Predict again\n",
    "- Measure decrease in accuracy\n",
    "\n",
    "SOMETHINGSOMETHINGSOMETHING[]\n",
    "\n",
    "## Cross Validation\n",
    "5-fold: for each of 5 cuts, hold 1 and use the other 4 as training (you can also take part of it and use it to tune hyperparameters)\n",
    "\n",
    "Eventually wage it against your validation set.\n",
    "\n",
    "### Things to be concerned about\n",
    "- How do you aggregate the parameters from different CVs?\n",
    "- What if hyperparameters are extremely variable? (Use more n-folds)\n",
    "- What if the hyperparameters are at the border of your grid search window? (Redo w/ values closer to window)\n",
    "\n",
    "**Don't** do feature-selection before cv fold selection. Let your CV pick which features are predictive.\n",
    "\n",
    "CVs on small samples predict lower performance, but may be underestimating this relative to the performance delivered? (CV is a key defense against overfitting)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Imbalanced Data\n",
    "\n",
    "Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Recommendation Systems](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afee45b9-dcf5-4f29-bc60-871aa78f1cf8)\n",
    "\n",
    "(really only gets into Recommender Systems by minute 30)\n",
    "\n",
    "#### Recommended Reading\n",
    "\n",
    "[Survey on Recommender Systems](http://files.grouplens/org/papers/FnT%20CF%20Recsys%20Survey.pdf)\n",
    "[Stanford Lecture (Slides)](http://web.stanford.edu/~lmackey/papers/cfslides-pml09.pdf)\n",
    "\n",
    "\n",
    "## Content-based filtering\n",
    "\n",
    "Uses similarity in attributes of items to recommend similar items to customers who liked it.\n",
    "\n",
    "Strengths: Less of a slow start\n",
    "\n",
    "Weakness: Need to maintain item descriptors\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "Matrix completion problem\n",
    "\n",
    "(ex: persons vs. object ratings)\n",
    "\n",
    "Assumes personal preferences are correlated\n",
    "\n",
    "Strengths: Finds new discoveries\n",
    "\n",
    "Weakness: Cold start problem\n",
    "\n",
    "#### CF as regression\n",
    "\n",
    "Pick algorithm, train predictor on each item, apply predictor to incompletes.\n",
    "\n",
    "(Problem: costly)\n",
    "\n",
    "#### KNN\n",
    "\n",
    "Widely used, can work against items or users\n",
    "\n",
    "Computes similarity between query user and all other users, find K most similar user's ratings, predict weighted average.\n",
    "\n",
    "Requires some thought into which similarity metric you should be using.\n",
    "\n",
    "### Similarity Measures\n",
    "\n",
    "#### Pearson Correlation Coefficient\n",
    "\n",
    "- Bounded between 1 and -1\n",
    "- Con: high similarity between users with few ratings in common (set minimum-threshold of n similar rating: recommended)\n",
    "\n",
    "$$s(u,v) = \\dfrac{\\sum_{i \\in I_u \\cap I_v}(r_{u,i}-\\bar{r_u})(r_{v,i}-\\bar{r_v})} {\\sqrt{\\sum_{i \\in I_u \\cap I_v}(r_{u,i}-\\bar{r_u})^2} \\sqrt{\\sum_{i \\in I_u \\cap I_v}(r_{v,i}-\\bar{r_v})^2}}$$\n",
    "\n",
    "(Go over this more carefully; what is $\\cap$)\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "As seen in Netflix recommender challenge (SimonFunk: [blog post explanation](http://sifter.org/~simon/journal/20061027.2.html)), caused a signficant gain in accuracy.\n",
    "\n",
    "(I'm going to go copy-paste my SVD notes from the RMarkdown file, see below)\n",
    "\n",
    "SimonFunk says SVD is a linear model. End result is a list of inferred categories sorted by relevance.\n",
    "\n",
    "Ullman book puts SVD in the category of dimensionality reduction (along with semantic indexing).\n",
    "\n",
    "\n",
    "IxU matrix A represented by $U \\sigma T^T$\n",
    "\n",
    "- $\\sigma$ is a kxk diagonal that shows the level of weight assigned to each Topic k\n",
    "- $U$ represents Users & Topics\n",
    "- $T^T$ represents Movies & Topics\n",
    "\n",
    "[Video explanation of SVD by Leskovec, Rajaraman, Ullman](https://www.youtube.com/watch?v=YKmkAolUxkU) (unavailable, aw. Can't find video, can find their book.)\n",
    "\n",
    "If you mean-center your data first, your $\\sigma$ contains the sqrt of your eigenvalues.\n",
    "\n",
    "(Can use SVD to compute PCA, and a lot of programs do this under the hood)\n",
    "\n",
    "## Map Reduce\n",
    "\n",
    "(associated with Spark)\n",
    "\n",
    "Programming model for addressing large datasets.\n",
    "- Paralell and distributed algorithms\n",
    "- Cluster framework (high overhead, needs to be failsafe)\n",
    "- Has associated way of thinking\n",
    "\n",
    "Originally developed by Google, Apache Hadoop is open-source implementation in Java. (MrJob is a Python interface to Hadoop from basic python files; no ipython interactivity.)\n",
    "\n",
    "### Map\n",
    "\n",
    "Performs filtering and sorting\n",
    "\n",
    "### Reduce\n",
    "\n",
    "Summary operation\n",
    "\n",
    "### Thought Process\n",
    "\n",
    "- Inputs: key-value pairs\n",
    "- Map it (part YOU determine!)\n",
    "- Shuffle & Sort (aggregate by keys)\n",
    "- Reduce (part YOU determine!)\n",
    "- Return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WordCount in Map Reduce\n",
    "###Counts word occurrences\n",
    "from mrjob.job import MRJob\n",
    "class mrWordCount(MRJob):\n",
    "    #Inherits from MRJob class\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "            yield word.lower(),1\n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "if __name__ == '__main__':\n",
    "    mrWordCount.run()\n",
    "\n",
    "#Launching this code in commandline\n",
    "python myscript.py < inputfile.txt > outputfile.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working up to Singular Value Decomposition\n",
    "\n",
    "**Complex Conjugate:** Expressed as $\\overline{A}$. Normal coordinates: $ax+bi \\rightarrow ax+(-b)i$. In polar coordinates: $\\rho e^{i\\phi} \\rightarrow \\rho e^{-i\\phi}$\n",
    "\n",
    "**[Transpose](https://en.wikipedia.org/wiki/Transpose):** Oft expressed as $A^T$  or $A^*$. Wiki also comes with succinct descriptions of Special Transpose Matrices.\n",
    "\n",
    "**Conjugate Transpose: **\n",
    "$\\overline{A^T}$\n",
    "\n",
    "$$A^\\dagger$$\n",
    "\n",
    "### **Unitary Matrix:**\n",
    "$U^\\dagger U = U U^\\dagger = I$\n",
    "\n",
    "### **Heritian Matrix:** \n",
    "- Complex Square Matrix\n",
    "- Equals own Conjugate Transpose \n",
    "\n",
    "$$A = A^\\dagger$$\n",
    "$$a_{ij} = \\overline{a_{ji}}$$\n",
    "\n",
    "## **Positive Semidefinite Hermitian Matrix:**\n",
    "for all $x \\in C^n$ : $x^\\dagger A x \\geq 0$\n",
    "\n",
    "#### **[Matrix Decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition):** Factorization of Matrices as a Product of Matrices \n",
    "\n",
    "### Eigendecomposition (aka Spectral Decomposition)\n",
    "\n",
    "aka factorization into canonical form\n",
    "\n",
    "$A = VDV^{-1}$\n",
    "\n",
    "- $A$: square matrix\n",
    "- $V$: cols are eigenvectors\n",
    "- $D$: diagonal formed from eigenvectors\n",
    "\n",
    "#### Eigenvectors\n",
    "\n",
    "Eigenvector of some transformation $T(v)$ is a (non-zero) vector $v$ that does not change direction under that linear transformation.\n",
    "\n",
    "$$T(v) = \\lambda v$$\n",
    "\n",
    "The **eigenvalue** is the scalar $lambda$.\n",
    "\n",
    "The linear transformation [in the case of matrices](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), is...\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "alternatively phrased as\n",
    "\n",
    "$$(A-\\lambda I)v = 0$$\n",
    "\n",
    "**Kernel: ** (Linear Algebra) for linear mapping $L$ that maps between vectorspaces $V$ and $W$,  $L : V -> W$, the kernel (aka nullspace) is the set of all elements v in V for which $L(v) = 0$ where 0 is the zero-vecotr in W. In set-builder notation...\n",
    "\n",
    "$$ker(L) = \\{ v \\in V | L(v) = 0 \\}$$\n",
    "\n",
    "\n",
    "## Matrix Polar Decomposition\n",
    "$A = UP$\n",
    "\n",
    "- $A$ is a square matrix\n",
    "- $U$ is a Unitary Matrix\n",
    "- $P$ is a Positive Semidefinite Hermitian Matrix\n",
    "\n",
    "# Singular Value Decomposition\n",
    "\n",
    "Uses Polar Decomposition to generalize Eigendecomposition to any $mxn$ matrix\n",
    "$$SVD( M) = UDV = U \\Sigma V^*$$\n",
    "\n",
    "UDV is notation in R; the other is as written on Wikipedia\n",
    "\n",
    "- $U$: Unitary matrix (known as Left Singular Value)\n",
    "- $\\Sigma$: Diagonal\n",
    "- $V^*$: Unitary matrix (known as Right Singular Value)\n",
    "- Both Singular Values: $\\sqrt{eigenvalue \\ \\lambda}$ of $M^*M$ and $MM^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sklearn resources\n",
    "[Basics](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "[1 hour](https://github.com/ogrisel/sklearn_pycon2014)\n",
    "[2 hour](https://www.youtube.com/watch?v=HjAB45qsx_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "- Be careful\n",
    "- Do not leak into the test data\n",
    "- Consider whether it's useful (mean? what about sd?)\n",
    "\n",
    "Normalize the validation and test sets using metrics from the training data.\n",
    "\n",
    "### Missing data?\n",
    "- Delete it\n",
    "    - Shrinks sample size\n",
    "- Use mean\n",
    "    - Doesn't change mean, is independent of other features (may weaken perceived links between them?)\n",
    "- Use regresssion to estimate\n",
    "    - Values will be deterministic\n",
    "\n",
    "(She works with Electron Microscopy data? Cool!)\n",
    "\n",
    "### Imbalanced Data\n",
    "(Imbalanced classes where one outcome is far more likely than the other, ex: one class is simply more likely, or there are filters on the data you see)\n",
    "Fixes?\n",
    "\n",
    "- Subsampling\n",
    "    - Use a subset of the larger category to train each tree in a random forest, while reusing the whole smaller category's data.\n",
    "- Oversampling\n",
    "- Re-weight sample points\n",
    "    - Make catching the rare instances relatively more important\n",
    "- Use clustering to reduce majority class\n",
    "- Re-calibrate classifier output\n",
    "- Beware easy true negatives!\n",
    "\n",
    "Imbalanced Data and Cross-Validation\n",
    "- Stratified sampling to generate folds (subsample the classes separately and maintain ratios)\n",
    "- Goal: same class ratio in training, validate, and test\n",
    "\n",
    "(check ROC curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Andrew Ng Week 9?\n",
    "\n",
    "# Anomaly Detection Algorithms\n",
    "\n",
    "## Density Estimation\n",
    "\n",
    "(Normal model of error in an anomaly detection algorithm that he says works just fine in many non-normal circumstances)\n",
    "\n",
    "$$p(x) = \\Pi_{j=1}^{n} \\left( x_j ; N(\\mu_j , \\sigma_j^2) \\right)$$\n",
    "\n",
    "Come up with threshold of error at which point things are dubbed anomalous.\n",
    "\n",
    "## Anomaly detection vs. Supervised Learning\n",
    "\n",
    "### Anomaly detection preferred when:\n",
    "- Very small # of positive examples (0-20 instances)\n",
    "- Large number of negative examples\n",
    "- Many different types of anomalies\n",
    "\n",
    "Ex: aircraft engines with a few duds\n",
    "\n",
    "### Supervised learning preferred when:\n",
    "- Large number of positive and negative examples\n",
    "- Shared character of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORGANIZATIONAL THINGS\n",
    "\n",
    "Some additional readings I should probably self-assign:\n",
    "- **Probabilistic Programming & Bayesian Methods for Hackers**\n",
    "- SVN Math notes by Andrew Ng\n",
    "- Skim: Chih-Wei Hsu et al., \"A Practical Guide to Support Vector Classification\", Bioinformatics(2010)\n",
    "\n",
    "Signal Prep: See Below\n",
    "\n",
    "\n",
    "\n",
    "Sanity checks and scheduling:\n",
    "- Plot out Signal prep-work, skim to assess which is redundant and which is new, plan to get it done before the bootcamp starts.\n",
    "- Schedule practincing w/ Fermi Modeling flashcards\n",
    "- Sanity-check: Will I be able to get up early enough to handle both Signal and feeding EVN's cats?\n",
    "- Considerations for final project: Research, decide what you want to demonstrate, pool together a Markdown reference sheet of relevant resources, brainstorm, sift and sort, reduce, rank (jointly consider potential Springboard and Signal projects)\n",
    "- Does Workflowy interact w/ email? Can I start using it as a decent productivity tool? Really should try, now that it has date-alerts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EVN cat sanity check: It's probably worth it if it means I don't go to mom's on weekends. I need to check exactly when Signal teaching actually starts, and what the teaching-mode really is. Deal with it super-early on the first day, and establish a habit of waking up early and walking before this activates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-course Signal Activities\n",
    "## Basics\n",
    "- ~~Set up email filters~~\n",
    "- Set up a Kaggle account\n",
    "- Send resume and Github username to Signal\n",
    "- Join Slack\n",
    "- ~~Join mailing list (I'm probably already in it)~~\n",
    "\n",
    "## Basic Programming:\n",
    "- Unix: Work through the The Unix Shell section of the Software Carpentry BIDS tutorials.\n",
    "- Python: Start by going through the Codecademy Python course. Afterward, work through sections 1–8 of the official Python tutorial, typing out each code sample by hand.\n",
    "- R: Watch the first 17 Google Developers Intro to R videos (less than 1 hour total). Next, work through DataCamp’s Kaggle R Tutorial.\n",
    "\n",
    "(See links in email)\n",
    "\n",
    "If you complete the above materials and would like to have more to work on, you can go through Wickham’s Advanced R or do a small Python project.\n",
    "\n",
    "## Readings\n",
    "Read through the following documents about data science in the provided order. I recommend spreading these out over the course of a week (some are longer than others).\n",
    "\n",
    "- Ch. 1 of Practical Data Science with R\n",
    "- Ch. 2 of Introduction to Statistical Learning (work through the R lab too)\n",
    "- A Few Useful Things to Know about Machine Learning\n",
    "- No, Really, Some of My Best Friends Are Data Scientists\n",
    "- Data Scientist: The Sexiest Job of the 21st Century\n",
    "- How to Break Into the Tech Industry—a Guide to Job Hunting and Tech Interviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
